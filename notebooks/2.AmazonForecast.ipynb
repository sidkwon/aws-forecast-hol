{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Forecast\n",
    "\n",
    "Black Friday (매년 11월 말)시점의 sales를 예측해 보자.\n",
    "\n",
    " - dept_id FOODS_3 (8230개) 중 200개 Sampling (Best200)\n",
    " - Target Time Series\n",
    "     - From/To : 2013-11-16/2015-11-15 (365*2일)\n",
    "     - timestamp (timestamp)\n",
    "     - id (string)\n",
    "     - sales (float)\n",
    " - Related Time Series\n",
    "     - From/To : 2013-11-16/2015-12-15 (365*2일 + 30일)\n",
    "     - timestamp (timestamp)\n",
    "     - id (string)\n",
    "     - sell_price (float)\n",
    "     - snap_CA, snap_TX, snap_WI (float)\n",
    "     - Easter, LaborDay, Purim_End, StPatricksDay, SuperBowl (float)\n",
    "     - Black Friday (float)\n",
    " - Item meta data : id, item_id, dept_id, cat_id, store_id, state_id\n",
    " - BackTestWindows : 4\n",
    " - BackTestWindowOffset : Default (Same as ForecastHorizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/forecast-steps.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import os\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import cycle\n",
    "import json\n",
    "import time\n",
    "from time import sleep\n",
    "import warnings\n",
    "\n",
    "color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (12,5)\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "plt.rcParams['lines.color'] = 'r'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "# Set maximum number of lines\n",
    "pd.set_option ('display.max_rows', 500)\n",
    "# Set the maximum number of columns\n",
    "pd.set_option ('display.max_columns', 500)\n",
    "# Width to display\n",
    "pd.set_option ('display.width', 1000)\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_merged) # 15,743,990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sales_foods_3) #8,230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_foods_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best200 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_로 시작하는 column 추출\n",
    "d_cols = [c for c in df_sales_foods_3.columns if 'd_' in c]\n",
    "\n",
    "# d_로 시작하는 column의 value(판매량)들을 더해 \"sales_total\" column에 추가\n",
    "df_sales_foods_3[\"sales_total\"] = df_sales_foods_3.loc[:,d_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily sales가 가장 많은 item Best200 list 선택\n",
    "best200  = df_sales_foods_3.sort_values(by=\"sales_total\", ascending=False).head(200)\n",
    "sampled = best200\n",
    "len(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled[[\"id\", \"sales_total\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled[[\"id\", \"sales_total\"]].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best200 추출\n",
    "df_merged_sampled = df_merged[df_merged[\"id\"].isin(sampled.id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_merged_sampled[\"id\"].unique()) # 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_sampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target (df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2013-11-16 ~ 2015-11-15\n",
    "df_target = df_merged_sampled[[\"id\", \"sales\"]]\n",
    "df_target = df_target.loc[\"2013-11-16\":\"2015-11-15\"] # 2year\n",
    "df_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_target) #146,000 = 200*365*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = df_target.sort_values(by=[\"id\", \"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related (df_related)\n",
    "- Black Friday 전일, 당일, 다음 날을 df_related 데이터에 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_related.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_related['black_friday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_sampled['black_friday'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2013년 Black Friday : 2013-11-29\n",
    "df_merged_sampled['black_friday'].loc[\"2013-11-28\":\"2013-11-30\"] = 1 \n",
    "\n",
    "# 2014년 Black Friday : 2013-11-28\n",
    "df_merged_sampled['black_friday'].loc[\"2014-11-27\":\"2014-11-29\"] = 1\n",
    "\n",
    "# 2015년 Black Friday : 2013-11-27\n",
    "df_merged_sampled['black_friday'].loc[\"2015-11-26\":\"2015-11-28\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2013-11-16 ~ 2015-12-15\n",
    "\n",
    "df_related = df_merged_sampled[[\"id\", \"event_name_1\", \"snap_CA\", \"snap_TX\", \"snap_WI\", \"sell_price\", \"black_friday\"]]\n",
    "\n",
    "# Related TS는 Target TS + ForecastHorizon까지 데이터가 있어야 하고,\n",
    "# Missing Value가 있으면 안된다.\n",
    "df_related = df_related.loc[\"2013-11-16\":\"2015-12-15\"]\n",
    "\n",
    "print(len(df_related)) # 152,000 = 200*(365*2+30)\n",
    "df_related.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_name_1의 NaN를 \"None\"으로 fill\n",
    "df_related[\"event_name_1\"] = df_related[\"event_name_1\"].fillna(\"None\")\n",
    "\n",
    "# 특정 item이 2015-07-01 이후부터 판매 되었다고 한다면, df_sales의 해당 item의 sell_price 데이터는 2015-07-01 이후부터 있을 것이다.\n",
    "# df_merged의 date는 df_calendar를 merge했으므로 특정 item의 date는 2011-01-29~2016-06-19 범위지만,\n",
    "# 특정 item의 df_sales내 date는 2015-07-01 이후이므로\n",
    "# df_merged와 df_sales를 Merge하면 특정 item의 2015-07-01 이전 시점의 sell_price는 NaN이다.\n",
    "# 따라서 sell_price의 NaN를 \"0\"으로 fill\n",
    "df_related[\"sell_price\"] = df_related[\"sell_price\"].fillna(0)\n",
    "df_related.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_related)) # 152,000 = 200*(365*2+30)\n",
    "df_related.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for event_name_1\n",
    "df_related = pd.concat([df_related, pd.get_dummies(df_related['event_name_1'])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_related)) # 39,500 = 100*(365+30)\n",
    "df_related.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_name_1에서 Unique value 추출\n",
    "all_events = df_related.event_name_1.unique()\n",
    "\n",
    "# event_name_1 : SuperBowl, LaborDay, Purim End, Easter, StPatricksDay <- Sales가 가장 많은 event 다섯 개만 Related에 추가\n",
    "chosen_events = ['SuperBowl', 'LaborDay', 'Purim End', 'Easter', 'StPatricksDay']\n",
    "for event in [event for event in all_events if event not in chosen_events]:\n",
    "    df_related.drop([event], axis=1, inplace=True)\n",
    "\n",
    "df_related.drop([\"event_name_1\"], axis=1, inplace=True)\n",
    "#df_related.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_related)) # 325,085 = 823*(365+30)\n",
    "df_related.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_related = df_related.sort_values(by=[\"id\", \"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item_metadata (df_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item = df_merged_sampled[[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_item) #200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_item[\"id\"].unique()) # 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare csv files\n",
    "\n",
    "!mkdir ./train\n",
    "\n",
    "local_path = \"./train/\"\n",
    "\n",
    "target_file_name     = \"df_target.csv\"\n",
    "related_file_name    = \"df_related.csv\"\n",
    "item_file_name       = \"df_item.csv\"\n",
    "\n",
    "local_target     = local_path + target_file_name\n",
    "local_related    = local_path + related_file_name\n",
    "local_item       = local_path + item_file_name\n",
    "\n",
    "df_target.to_csv(local_target, header=False, index=True)\n",
    "df_related.to_csv(local_related, header=False, index=True)\n",
    "df_item.to_csv(local_item, header=False, index=False) #index 제외"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast 시작\n",
    "\n",
    "참고 : https://github.com/chrisking/ForecastPOC/blob/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FREQUENCY = \"D\" # Day\n",
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd\"\n",
    "\n",
    "project = 'walmart_m5'\n",
    "datasetName= project+'_ds'\n",
    "datasetGroupName= project +'_dsg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook이 실행되는 AWS region 정보 추출\n",
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=region)\n",
    "forecast = session.client(service_name='forecast')\n",
    "forecast_query = session.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sagemaker Jupyter notebook에서 Amazon Forecast의 API를 사용할 수 있도록 execution_role을 가져 온다.\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role_arn = get_execution_role()\n",
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datagroup 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_group_response = forecast.create_dataset_group(DatasetGroupName=datasetGroupName,\n",
    "                                                              Domain=\"RETAIL\",\n",
    "                                                             )\n",
    "datasetGroupArn = create_dataset_group_response['DatasetGroupArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.describe_dataset_group(DatasetGroupArn=datasetGroupArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Target Time Series Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the schema of your dataset here. Make sure the order of columns matches the raw data files.\n",
    "schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"demand\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_DSN = datasetName + \"_target\"\n",
    "\n",
    "response=forecast.create_dataset(\n",
    "                    Domain=\"RETAIL\",\n",
    "                    DatasetType='TARGET_TIME_SERIES',\n",
    "                    DatasetName=target_DSN,\n",
    "                    DataFrequency=DATASET_FREQUENCY, \n",
    "                    Schema = schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_datasetArn = response['DatasetArn']\n",
    "forecast.describe_dataset(DatasetArn=target_datasetArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Target Time Series Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 Bucket\n",
    "# {Account Number}-forecastpoc\n",
    "\n",
    "print(region)\n",
    "s3 = boto3.client('s3')\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket_name = account_id + \"-forecastpoc\"\n",
    "print(bucket_name)\n",
    "s3.create_bucket(Bucket=bucket_name)\n",
    "if region != \"us-east-1\":\n",
    "    s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': region})\n",
    "else:\n",
    "    s3.create_bucket(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Target File\n",
    "\n",
    "bucket_name = bucket_name\n",
    "role_arn = role_arn\n",
    "\n",
    "s3_path = \"walmart\"\n",
    "\n",
    "s3_target     = \"s3://\" + bucket_name + \"/\" + s3_path + \"/\" + target_file_name\n",
    "s3_related    = \"s3://\" + bucket_name + \"/\" + s3_path + \"/\" + related_file_name\n",
    "s3_item       = \"s3://\" + bucket_name + \"/\" + s3_path + \"/\" + item_file_name\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(s3_path + \"/\" + target_file_name).upload_file(local_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can call import the dataset\n",
    "role_arn = role_arn #ForecastRolePOC\n",
    "datasetImportJobName = 'DSIMPORT_JOB_TARGET_POC'\n",
    "ds_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName=datasetImportJobName,\n",
    "                                                          DatasetArn=target_datasetArn,\n",
    "                                                          DataSource= {\n",
    "                                                              \"S3Config\" : {\n",
    "                                                                 \"Path\":s3_target,\n",
    "                                                                 \"RoleArn\": role_arn\n",
    "                                                              } \n",
    "                                                          },\n",
    "                                                          TimestampFormat=TIMESTAMP_FORMAT\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_import_job_arn=ds_import_job_response['DatasetImportJobArn']\n",
    "print(ds_import_job_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while True:\n",
    "#    dataImportStatus = forecast.describe_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)['Status']\n",
    "#    print(dataImportStatus)\n",
    "#    if dataImportStatus != 'ACTIVE' and dataImportStatus != 'CREATE_FAILED':\n",
    "#        sleep(30)\n",
    "#    else:\n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방금 만든 dataset을 dataset group에 attach한다.\n",
    "# attach하지 않으면 Forecast dataset group의 dataset가 조회되지 않는다.\n",
    "#response = forecast.update_dataset_group(\n",
    "#    DatasetGroupArn=datasetGroupArn,\n",
    "#    DatasetArns=[\n",
    "#        target_datasetArn\n",
    "#    ]\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. Related Time Series dataset 생성\n",
    "\n",
    "Related Time Series 고려사항 : https://docs.aws.amazon.com/ko_kr/forecast/latest/dg/related-time-series-datasets.html\n",
    "\n",
    "<img src=\"../img/related-ts.png\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Related File\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(s3_path + \"/\" + related_file_name).upload_file(local_related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_related.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the schema of your dataset here. Make sure the order of columns matches the raw data files.\n",
    "related_schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"snap_CA\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"snap_TX\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"snap_WI\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"sell_price\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"black_friday\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"Easter\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"LaborDay\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"Purim_End\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"StPatricksDay\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"SuperBowl\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_DSN = datasetName + \"_related\"\n",
    "response=forecast.create_dataset(\n",
    "                    Domain=\"RETAIL\",\n",
    "                    DatasetType='RELATED_TIME_SERIES',\n",
    "                    DatasetName=related_DSN,\n",
    "                    DataFrequency=DATASET_FREQUENCY, \n",
    "                    Schema = related_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_datasetArn = response['DatasetArn']\n",
    "print(related_datasetArn)\n",
    "forecast.describe_dataset(DatasetArn=related_datasetArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d. Related Time Series Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetImportJobName = 'DSIMPORT_JOB_RELATEDPOC_2'\n",
    "related_ds_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName=datasetImportJobName,\n",
    "                                                          DatasetArn=related_datasetArn,\n",
    "                                                          DataSource= {\n",
    "                                                              \"S3Config\" : {\n",
    "                                                                 \"Path\":s3_related,\n",
    "                                                                 \"RoleArn\": role_arn\n",
    "                                                              } \n",
    "                                                          },\n",
    "                                                          TimestampFormat=TIMESTAMP_FORMAT\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_ds_import_job_arn=related_ds_import_job_response['DatasetImportJobArn']\n",
    "print(rel_ds_import_job_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while True:\n",
    "#    dataImportStatus = forecast.describe_dataset_import_job(DatasetImportJobArn=rel_ds_import_job_arn)['Status']\n",
    "#    print(dataImportStatus)\n",
    "#    if dataImportStatus != 'ACTIVE' and dataImportStatus != 'CREATE_FAILED':\n",
    "#        sleep(30)\n",
    "#    else:\n",
    "#        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2e. Item Metadata 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Item Metadata File\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(s3_path + \"/\" + item_file_name).upload_file(local_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"item_id_not_combined\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"dept_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"cat_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"store_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"state_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_DSN = datasetName + \"_item\"\n",
    "response=forecast.create_dataset(\n",
    "                    Domain=\"RETAIL\",\n",
    "                    DatasetType='ITEM_METADATA',\n",
    "                    DatasetName=item_DSN,\n",
    "                    Schema = item_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_datasetArn = response['DatasetArn']\n",
    "print(item_datasetArn)\n",
    "forecast.describe_dataset(DatasetArn=item_datasetArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2f. Item Metadata Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetImportJobName = 'DSIMPORT_JOB_ITEMPOC'\n",
    "item_ds_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName=datasetImportJobName,\n",
    "                                                          DatasetArn=item_datasetArn,\n",
    "                                                          DataSource= {\n",
    "                                                              \"S3Config\" : {\n",
    "                                                                 \"Path\":s3_item,\n",
    "                                                                 \"RoleArn\": role_arn\n",
    "                                                              } \n",
    "                                                          }\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ds_import_job_arn=item_ds_import_job_response['DatasetImportJobArn']\n",
    "print(item_ds_import_job_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2g. Check Dataset Import Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    TargetdataImportStatus  = forecast.describe_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)['Status']\n",
    "    RelateddataImportStatus = forecast.describe_dataset_import_job(DatasetImportJobArn=rel_ds_import_job_arn)['Status']\n",
    "    ItemdataImportStatus    = forecast.describe_dataset_import_job(DatasetImportJobArn=item_ds_import_job_arn)['Status']\n",
    "    \n",
    "    print(\"Dataset {} status : {}\".format(target_datasetArn, TargetdataImportStatus))\n",
    "    print(\"Dataset {} status : {}\".format(related_datasetArn, RelateddataImportStatus))\n",
    "    print(\"Dataset {} status : {}\".format(item_datasetArn, ItemdataImportStatus))\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    \n",
    "    if TargetdataImportStatus != 'ACTIVE' or RelateddataImportStatus != 'ACTIVE' or ItemdataImportStatus != 'ACTIVE':\n",
    "        sleep(30)\n",
    "    else:\n",
    "        break\n",
    "print('작업 수행된 시간 : %f 초' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = forecast.update_dataset_group(\n",
    "    DatasetGroupArn=datasetGroupArn,\n",
    "    DatasetArns=[\n",
    "        target_datasetArn,\n",
    "        related_datasetArn,\n",
    "        item_datasetArn\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 스크린 캡쳐와 같이 3가지 Dataset이 모두 Import되었는지 확인한 후 \"Create Predictor\" 단계로 넘어 간다.\n",
    "Import 상태가 \"Falied\"인 경우 세부 오류 메시지를 확인한다.\n",
    "\n",
    "<img src=\"../img/datasets.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Predictor (20~30분 소요)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastHorizon = 30 # 30 days\n",
    "NumberOfBacktestWindows = 4\n",
    "BackTestWindowOffset = 30\n",
    "ForecastFrequency = \"D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_algorithmArn = 'arn:aws:forecast:::algorithm/Prophet'\n",
    "deepAR_Plus_algorithmArn = 'arn:aws:forecast:::algorithm/Deep_AR_Plus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet Specifics\n",
    "prophet_predictorName= project+'_prophet_algo_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Prophet:\n",
    "prophet_create_predictor_response=forecast.create_predictor(PredictorName=prophet_predictorName, \n",
    "                                                  AlgorithmArn=prophet_algorithmArn,\n",
    "                                                  ForecastHorizon=forecastHorizon,\n",
    "                                                  PerformAutoML= False,\n",
    "                                                  PerformHPO=False,\n",
    "                                                  EvaluationParameters= {\"NumberOfBacktestWindows\": NumberOfBacktestWindows, \n",
    "                                                                         \"BackTestWindowOffset\": BackTestWindowOffset}, \n",
    "                                                  InputDataConfig= {\"DatasetGroupArn\": datasetGroupArn, \"SupplementaryFeatures\": [ \n",
    "                                                                     { \n",
    "                                                                        \"Name\": \"holiday\",\n",
    "                                                                        \"Value\": \"US\"\n",
    "                                                                     }\n",
    "                                                                  ]},\n",
    "                                                  FeaturizationConfig= {\"ForecastFrequency\": ForecastFrequency, \n",
    "                                                                        \"Featurizations\": \n",
    "                                                                        [\n",
    "                                                                          {\"AttributeName\": \"demand\", \n",
    "                                                                           \"FeaturizationPipeline\": \n",
    "                                                                            [\n",
    "                                                                              {\"FeaturizationMethodName\": \"filling\", \n",
    "                                                                               \"FeaturizationMethodParameters\": \n",
    "                                                                                {\"frontfill\": \"none\", \n",
    "                                                                                 \"middlefill\": \"zero\", \n",
    "                                                                                 \"backfill\": \"zero\"}\n",
    "                                                                              }\n",
    "                                                                            ]\n",
    "                                                                          }\n",
    "                                                                        ]\n",
    "                                                                       }\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. DeepAR Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet Specifics\n",
    "deeparplus_predictorName= project+'_deeparplus_algo_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DeepAR Plus:\n",
    "deeparplus_create_predictor_response=forecast.create_predictor(PredictorName=deeparplus_predictorName, \n",
    "                                                  AlgorithmArn=deepAR_Plus_algorithmArn,\n",
    "                                                  ForecastHorizon=forecastHorizon,\n",
    "                                                  PerformAutoML= False,\n",
    "                                                  PerformHPO=False,\n",
    "                                                  EvaluationParameters= {\"NumberOfBacktestWindows\": NumberOfBacktestWindows, \n",
    "                                                                         \"BackTestWindowOffset\": BackTestWindowOffset}, \n",
    "                                                  InputDataConfig= {\"DatasetGroupArn\": datasetGroupArn, \"SupplementaryFeatures\": [ \n",
    "                                                                     { \n",
    "                                                                        \"Name\": \"holiday\",\n",
    "                                                                        \"Value\": \"US\"\n",
    "                                                                     }\n",
    "                                                                  ]},\n",
    "                                                  FeaturizationConfig= {\"ForecastFrequency\": ForecastFrequency, \n",
    "                                                                        \"Featurizations\": \n",
    "                                                                        [\n",
    "                                                                          {\"AttributeName\": \"demand\", \n",
    "                                                                           \"FeaturizationPipeline\": \n",
    "                                                                            [\n",
    "                                                                              {\"FeaturizationMethodName\": \"filling\", \n",
    "                                                                               \"FeaturizationMethodParameters\": \n",
    "                                                                                {\"frontfill\": \"none\", \n",
    "                                                                                 \"middlefill\": \"zero\", \n",
    "                                                                                 \"backfill\": \"zero\"}\n",
    "                                                                              }\n",
    "                                                                            ]\n",
    "                                                                          }\n",
    "                                                                        ]\n",
    "                                                                       },\n",
    "                                                 TrainingParameters= { \n",
    "                                                          \"likelihood\" : \"negative-binomial\" \n",
    "                                                       }\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적으로 Prophet predictor 학습은 DeepAR+ 보다 빨리 끝난다.\n",
    "- Prophet predictor 학습이 완료되어 predictor status가 `ACTIVE`인 경우 Prophet predictor를 이용하여 Forecast를 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c. Check Predictor Creation Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    ProphetArn = prophet_create_predictor_response['PredictorArn']\n",
    "    #ProphetArn = \"arn:aws:forecast:us-east-1:889750940888:predictor/walmart_prophet_algo_1\"\n",
    "    DeepARPlusArn = deeparplus_create_predictor_response['PredictorArn']\n",
    "    #DeepARPlusArn = \"arn:aws:forecast:us-east-1:889750940888:predictor/walmart_deeparplus_algo_1\"\n",
    "    \n",
    "    ProphetStatus = forecast.describe_predictor(PredictorArn = prophet_create_predictor_response['PredictorArn'])['Status']\n",
    "    DeepARPlusStatus = forecast.describe_predictor(PredictorArn = deeparplus_create_predictor_response['PredictorArn'])['Status']\n",
    "    #DeepARPlusStatus = forecast.describe_predictor(PredictorArn = DeepARPlusArn)['Status']\n",
    "    \n",
    "    print(\"Predictor {} status : {}\".format(ProphetArn, ProphetStatus))\n",
    "    print(\"Predictor {} status : {}\".format(DeepARPlusArn, DeepARPlusStatus))\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    \n",
    "    if ProphetStatus != 'ACTIVE' or DeepARPlusStatus != 'ACTIVE':\n",
    "        sleep(30)\n",
    "    else:\n",
    "        break\n",
    "print('작업 수행된 시간 : %f 초' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d. Examining the Predictors\n",
    "- AWS Forecast에서 생성된 Predictor별 Metric을 확인한다.\n",
    "- 참고 : https://docs.aws.amazon.com/ko_kr/forecast/latest/dg/metrics.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create Forecast\n",
    "- Predictor별 Forecast를 만든다.\n",
    "- 5분 ~ 10분 소요\n",
    "- 참고 : https://docs.aws.amazon.com/ko_kr/forecast/latest/dg/gs-console.html 의 \"3단계 - 예상 생성\"\n",
    "- ForecastTypes : The quantiles at which probabilistic forecasts are generated. You can currently specify up to 5 quantiles per forecast. Accepted values include 0.01 to 0.99 (increments of .01 only) and mean. The mean forecast is different from the median (0.50) when the distribution is not symmetric (for example, Beta and Negative Binomial). The default value is [\"0.1\", \"0.5\", \"0.9\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Create Prophet, DeepAR+ Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeparplus_forecastName= project+'_deepAR_algo_forecast'\n",
    "prophet_forecastname= project+'_prophet_algo_forecast'\n",
    "ForecastTypes=[\"0.1\", \"0.5\", \"0.9\", \"mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DeepAR+**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_response=forecast.create_forecast(ForecastName=prophet_forecastname,\n",
    "                                                  ForecastTypes=ForecastTypes,\n",
    "                                                  PredictorArn=ProphetArn\n",
    "                                                 )\n",
    "prophet_forecastArn = create_forecast_response['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeparplus_forecastName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prophet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_response=forecast.create_forecast(ForecastName=deeparplus_forecastName,\n",
    "                                                  ForecastTypes=ForecastTypes,\n",
    "                                                  PredictorArn = DeepARPlusArn\n",
    "                                                 )\n",
    "deeparplus_forecastArn = create_forecast_response['ForecastArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_forecastArn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Check Forecast Creation Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    deeparplus_forecast_status = forecast.describe_forecast(ForecastArn=deeparplus_forecastArn)['Status']\n",
    "    prophet_forecast_status = forecast.describe_forecast(ForecastArn=prophet_forecastArn)['Status']\n",
    "    \n",
    "    print(\"Predictor {} status : {}\".format(deeparplus_forecastArn, deeparplus_forecast_status))\n",
    "    print(\"Predictor {} status : {}\".format(prophet_forecastArn, prophet_forecast_status))\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    \n",
    "    if deeparplus_forecast_status != 'ACTIVE' or prophet_forecast_status != 'ACTIVE':\n",
    "        sleep(30)\n",
    "    else:\n",
    "        break\n",
    "print('작업 수행된 시간 : %f 초' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c. Get Forecast & Visualization\n",
    "Predictor별 Forecast를 생성한 후 id별 p10, p50, p90, mean 값을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200개의 Sample item 중 top10\n",
    "sampled[[\"id\", \"sales_total\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200개의 Sample item 중 worst10\n",
    "sampled[[\"id\", \"sales_total\"]].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forecast(id):\n",
    "    for forecastArn in [deeparplus_forecastArn, prophet_forecastArn]:\n",
    "        forecastResponse = forecast_query.query_forecast(\n",
    "                            ForecastArn=forecastArn,\n",
    "                            Filters={\"item_id\":id}\n",
    "                            )\n",
    "\n",
    "        mean = pd.DataFrame(forecastResponse['Forecast']['Predictions']['mean'])\n",
    "        mean.Timestamp = mean.Timestamp.apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S\"))\n",
    "        mean.set_index(\"Timestamp\", inplace=True)\n",
    "        mean.rename(columns = {'Value' : 'mean'}, inplace = True)\n",
    "\n",
    "        p10 = pd.DataFrame(forecastResponse['Forecast']['Predictions']['p10'])\n",
    "        p10.Timestamp = p10.Timestamp.apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S\"))\n",
    "        p10.set_index(\"Timestamp\", inplace=True)\n",
    "        p10.rename(columns = {'Value' : 'p10'}, inplace = True)\n",
    "\n",
    "        p50 = pd.DataFrame(forecastResponse['Forecast']['Predictions']['p50'])\n",
    "        p50.Timestamp = p50.Timestamp.apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S\"))\n",
    "        p50.set_index(\"Timestamp\", inplace=True)\n",
    "        p50.rename(columns = {'Value' : 'p50'}, inplace = True)\n",
    "\n",
    "        p90 = pd.DataFrame(forecastResponse['Forecast']['Predictions']['p90'])\n",
    "        p90.Timestamp = p90.Timestamp.apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S\"))\n",
    "        p90.set_index(\"Timestamp\", inplace=True)\n",
    "        p90.rename(columns = {'Value' : 'p90'}, inplace = True)\n",
    "\n",
    "        plot_start_ts = mean.index.min() - timedelta(days=0.5 * 365/12)\n",
    "        plot_end_ts   = mean.index.max() + timedelta(days=0.5 * 365/12)\n",
    "        plot_start_str = datetime.strptime(str(plot_start_ts), '%Y-%m-%d %H:%M:%S')\n",
    "        plot_end_str   = datetime.strptime(str(plot_end_ts), '%Y-%m-%d %H:%M:%S')\n",
    "        plot_start_date = str(plot_start_str.year) + \"-\" + str(plot_start_str.month) + \"-\" + str(plot_start_str.day)\n",
    "        plot_end_date   = str(plot_end_str.year) + \"-\" + str(plot_end_str.month) + \"-\" + str(plot_end_str.day)\n",
    "\n",
    "        observations = df_merged[df_merged[\"id\"] == id].loc[plot_start_date:plot_end_date].sales\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "        plt.title(\"Forecast for {}, Predictor : {}\".format(id, forecastArn))\n",
    "        plt.plot(observations, color='gray', linewidth=1, label=\"observation\")\n",
    "        plt.plot(p90, label='p90')\n",
    "        plt.plot(mean, label='mean')\n",
    "        plt.plot(p50, label='p50')\n",
    "        plt.plot(p10, label='p10')\n",
    "        plt.axvline(x=datetime(2015, 11, 27), color='r', linestyle='--', linewidth=3) # Adding Vertical line for Black Friday\n",
    "        plt.legend()\n",
    "     \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample중 Top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled[[\"id\", \"sales_total\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled.id.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in sampled.id.head(5):\n",
    "    get_forecast(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample중 Worst5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in sampled.id.tail(5):\n",
    "    get_forecast(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
